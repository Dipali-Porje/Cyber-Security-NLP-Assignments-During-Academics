{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOXKC6MZSxx/rXBXG+SXIEf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["NLP Assignment 4"],"metadata":{"id":"Q7cX5QHtCccq"}},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CpjBkI52AhFJ","executionInfo":{"status":"ok","timestamp":1740368075725,"user_tz":-330,"elapsed":14217,"user":{"displayName":"dipali porje","userId":"15697648671118973123"}},"outputId":"015b53df-eae2-4baf-aee5-5e70d2e63841"},"outputs":[{"output_type":"stream","name":"stdout","text":["Enter a comment: i will kill you\n","The comment contains a threat.\n"]}],"source":["# Function to check if a comment contains toxic words\n","def contains_toxic_word(comment, toxic_words):\n"," comment = comment.lower() # Convert comment to lowercase for case-insensitive matching\n"," for word in toxic_words:\n","   if word in comment:\n","     return True\n"," return False\n","def main():\n","  # Define lists of toxic words for each category\n","  threat_words = [\"kill\", \"shoot\", \"die\", \"murder\", \"attack\"]\n","  obscenity_words = [\"donkey\", \"shit\", \"bitch\", \"asshole\", \"cunt\"]\n","  insult_words = [\"stupid\", \"idiot\", \"moron\", \"dumb\", \"loser\"]\n","  hate_words = [\"nigger\", \"faggot\", \"bitch\", \"chink\", \"terrorist\"]\n","  # Get input from the user\n","  comment = input(\"Enter a comment: \")\n","  # Check for each type of toxicity\n","  is_toxic = False\n","\n","  # Check for threats\n","  if contains_toxic_word(comment, threat_words):\n","    print(\"The comment contains a threat.\")\n","    is_toxic = True\n","\n","  # Check for obscenity\n","  if contains_toxic_word(comment, obscenity_words):\n","    print(\"The comment contains obscenity.\")\n","    is_toxic = True\n","  # Check for insults\n","  if contains_toxic_word(comment, insult_words):\n","    print(\"The comment contains an insult.\")\n","    is_toxic = True\n","  # Check for hate speech\n","  if contains_toxic_word(comment, hate_words):\n","    print(\"The comment contains identity-based hate.\")\n","    is_toxic = True\n","  # If no toxicity is detected\n","  if not is_toxic:\n","    print(\"The comment is clean.\")\n","\n","# Run the program\n","if __name__ == \"__main__\":\n"," main()"]},{"cell_type":"markdown","source":["NLP Assignment 2"],"metadata":{"id":"FOKph8g7CRKi"}},{"cell_type":"code","source":["import re\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","from nltk.corpus import stopwords\n","import nltk"],"metadata":{"id":"5Z-7YuykAlpz","executionInfo":{"status":"ok","timestamp":1740368113876,"user_tz":-330,"elapsed":3046,"user":{"displayName":"dipali porje","userId":"15697648671118973123"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Download necessary resources\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","nltk.download('punkt_tab')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zZiQRIUhBeNZ","executionInfo":{"status":"ok","timestamp":1740368270719,"user_tz":-330,"elapsed":261,"user":{"displayName":"dipali porje","userId":"15697648671118973123"}},"outputId":"657dfa51-62cb-4fd5-8648-95a2213bdfe5"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["def preprocess_text(text):\n","  print(\"Original Text:\")\n","  print(text)\n","  print(\"\\n--- Preprocessing Steps ---\")\n","# 1. Tokenization\n","# Using NLTK\n","  word_tokens = word_tokenize(text)\n","  print(\"\\nWord Tokens (NLTK):\")\n","  print(word_tokens)\n","# Using Regular Expressions\n","  word_tokens_regex = re.findall(r'\\b\\w+\\b', text)\n","  print(\"\\nWord Tokens (Regex):\")\n","  print(word_tokens_regex)\n"," # 2. Stemming\n","  ps = PorterStemmer()\n","  stemmed_words = [ps.stem(word) for word in word_tokens]\n","  print(\"\\nStemmed Words (NLTK):\")\n","  print(stemmed_words)\n","# 3. Lemmatization\n","  lemmatizer = WordNetLemmatizer()\n","  lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in word_tokens]\n","  print(\"\\nLemmatized Words (NLTK):\")\n","  print(lemmatized_words)\n","# 4. Stop Word Removal\n","  stop_words = set(stopwords.words('english'))\n","  filtered_words = [word for word in word_tokens if word.lower() not in stop_words]\n","  print(\"\\nFiltered Words (No Stop Words, NLTK):\")\n","  print(filtered_words)\n","# Using Regular Expressions for Stop Word Removal\n","  stop_words_regex = r'\\b(?:is|the|and|in|on|at|a|an|of|to)\\b'\n","  filtered_text = re.sub(stop_words_regex, '', text, flags=re.IGNORECASE)\n","  filtered_text = re.sub(r'\\s+', ' ', filtered_text).strip()\n","  print(\"\\nFiltered Text (Regex):\")\n","  print(filtered_text)\n","# Example Input Text\n","sample_text = \"Natural Language Processing (NLP) helps in understanding human language efficiently.\"\n","preprocess_text(sample_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-WiSStqWBg7R","executionInfo":{"status":"ok","timestamp":1740368274879,"user_tz":-330,"elapsed":2212,"user":{"displayName":"dipali porje","userId":"15697648671118973123"}},"outputId":"d781d809-2076-43d0-d1d0-7969c5c7cebb"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Original Text:\n","Natural Language Processing (NLP) helps in understanding human language efficiently.\n","\n","--- Preprocessing Steps ---\n","\n","Word Tokens (NLTK):\n","['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'helps', 'in', 'understanding', 'human', 'language', 'efficiently', '.']\n","\n","Word Tokens (Regex):\n","['Natural', 'Language', 'Processing', 'NLP', 'helps', 'in', 'understanding', 'human', 'language', 'efficiently']\n","\n","Stemmed Words (NLTK):\n","['natur', 'languag', 'process', '(', 'nlp', ')', 'help', 'in', 'understand', 'human', 'languag', 'effici', '.']\n","\n","Lemmatized Words (NLTK):\n","['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'help', 'in', 'understand', 'human', 'language', 'efficiently', '.']\n","\n","Filtered Words (No Stop Words, NLTK):\n","['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'helps', 'understanding', 'human', 'language', 'efficiently', '.']\n","\n","Filtered Text (Regex):\n","Natural Language Processing (NLP) helps understanding human language efficiently.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"x2_YCkr5BoaB"},"execution_count":null,"outputs":[]}]}